% \documentclass{article}
\documentclass{article}
\usepackage{xeCJK}
\setCJKmainfont{SimSong}


\author{Weizhao Wang}
\date{\today}

\begin{document}

\title{Sorting Arrays with Transformer Models}

\maketitle

\begin{abstract}
    Nowedays, sorting algorithms are widely used in various applications, such as databases, search engines, and data analysis. In this paper, we propose a novel approach to sorting arrays using Transformer models, a type of neural network architecture that has shown remarkable performance in natural language processing tasks. We adapt the Transformer model to learn the sorting order of elements in an array, achieving competitive results compared to traditional sorting algorithms. Our experiments demonstrate the effectiveness of the proposed method across a range of array sizes and types. This research opens up new possibilities for leveraging machine learning techniques in sorting problems and has the potential to improve the efficiency of sorting algorithms in practice.
\end{abstract}

\section{Introduction}

Sorting is a fundamental operation in computer science and is used in a wide range of applications, such as databases, search engines, and data analysis. Traditional sorting algorithms, such as quicksort, mergesort, and heapsort, have been extensively studied and optimized over the years. These algorithms have excellent performance in practice and are widely used in various software systems. However, with the advent of deep learning and neural networks, there is growing interest in exploring alternative approaches to sorting using machine learning techniques.

In this paper, we propose a novel approach to sorting arrays using Transformer models, a type of neural network architecture that has shown remarkable performance in natural language processing tasks. The Transformer model, introduced by Vaswani et al. (2017), has achieved state-of-the-art results in machine translation, text generation, and other sequence-to-sequence tasks. The key innovation of the Transformer model is the self-attention mechanism, which allows the model to capture long-range dependencies in the input sequence efficiently.

We adapt the Transformer model to learn the sorting order of elements in an array. Given an input array, the model predicts the sorted order of the elements, producing a permutation that sorts the array in ascending order. We train the model on a large dataset of input-output pairs generated from random arrays, optimizing it to minimize the discrepancy between the predicted permutation and the ground truth sorting order. Our experiments demonstrate that the proposed method achieves competitive results compared to traditional sorting algorithms across a range of array sizes and types.

The rest of this paper is organized as follows. In Section 2, we provide background information on sorting algorithms and the Transformer model. In Section 3, we describe the architecture of our sorting model and the training procedure. In Section 4, we present the experimental results and analyze the performance of the model. Finally, in Section 5, we conclude the paper and\cite{vaswani2017attention} discuss future research directions.


\section{Background}


\section{Model Architecture}

\section{Training}

\section{Results}

\section{Conclusion}


% references
\begin{thebibliography}{9}
    \bibitem{vaswani2017attention}
    Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... \& Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008).
\end{thebibliography}

\end{document}
